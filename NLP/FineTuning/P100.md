---
layout: default
title: P100應用
parent: LLM 之微調
grand_parent: 自然語言處理
nav_order: 99
date: 2024-01-30
last_modified_date: 2024-01-30 05:27:53
tags: AI chat
---


# P100上的裝置與應用
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
---

## 背景

- 在单块16G的推理卡上微调复现vicuna-7b([git-cloner / llama-lora-fine-tuning ](https://github.com/git-cloner/llama-lora-fine-tuning/blob/main/README_cn.md))

以下是对您的内容的摘要：

- **微调目标**：在单块16G的推理卡上微调复现vicuna-7b，一种基于多轮对话语料的FaceBook/LLaMA的微调方案。
- **微调方案**：采用lora方式只训练一部分参数，基础模型采用半精度llama-7b-hf，使用load_in_8bit装载基础模型，采用peft技术微调，采用bitsandbytes加速。
- **微调过程**：分为安装依赖环境、准备Llama模型、整理语料和微调四个步骤，每个步骤都有详细的命令和说明。
- **参考链接**：本文参考了以下项目和文章：

    - [alpaca-lora](https://github.com/tloen/alpaca-lora)
    - [FastChat](https://github.com/lm-sys/FastChat)
    - [知乎](https://zhuanlan.zhihu.com/p/597063490)
    - [decapoda-research](https://huggingface.co/decapoda-research/llama-7b-hf)
    - [juncongmoo](https://github.com/juncongmoo/pyllama)
    - [ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)
    - [LLMDataHub](https://github.com/Zjh-819/LLMDataHub)
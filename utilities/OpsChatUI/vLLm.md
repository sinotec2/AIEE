

vLLM 是一個開源的大型語言模型（LLM）推理和服務庫，旨在提高效能並簡化LLM 的部署。 

它透過創新的方法，例如使用 PagedAttention 技術有效管理記憶體，減少碎片並提升吞吐量，其吞吐量甚至比傳統方法高出許多倍。 

vLLM 易於使用，支援多種硬體平台，並可作為OpenAI API 的替代方案部署。 

## 主要功能與優勢

- 高效能吞吐量： 採用PagedAttention、動態批次處理等技術，能大幅提升服務的吞吐量和響應速度。 
- 高效的記憶體管理： 透過將KV 快取分割成區塊進行精細管理，有效避免記憶體浪費和碎片化，實現接近最佳的記憶體使用率。 
- 靈活易用： 支援與各種模型無縫整合，並可部署在多種硬體平台（如NVIDIA、AMD、Intel 的GPU 和CPU）上。 
- 多種最佳化技術： 支援量化（GPTQ、AWQ、INT4、INT8、FP8）以及最佳化的CUDA 核心，進一步提升執行速度。 
- 易於整合： 能夠作為實作OpenAI API 的伺服器，方便現有應用程式的整合和切換。 

## 核心技術

### PagedAttention

這項核心技術將LLM 的注意力鍵值(KV) 快取記憶體分割成固定大小的頁面，類似於虛擬記憶體。 

這讓vLLM 能夠在需要時才分配實體GPU 記憶體，從而消除了記憶體碎片，並減少了記憶體浪費。

### 連續批次處理

連續批次處理 (Continuous Batching)在處理這類同時進來的請求，vLLM 會在處理請求時動態地將它們組合在一起，而不是等待所有請求都準備好才進行批次處理。這提高了GPU 的利用率。 



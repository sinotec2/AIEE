
#!
# -*- coding: utf-8 -*-

"""
Ollama Docker ç›£æ§è…³æœ¬

Author      : yckuang
Version     : 1.1.0
License     : MIT

åŠŸèƒ½èªªæ˜
---------
æ­¤è…³æœ¬æœƒå®šæœŸæª¢æŸ¥ GPU æ˜¯å¦è¢« Ollama æœå‹™ä½”ç”¨ï¼Œ
è‹¥ GPU ç„¡ä½¿ç”¨æˆ–å‡ºç¾é•·æœŸå¡é “ï¼Œå‰‡è‡ªå‹•é‡æ–°éƒ¨ç½²
æŒ‡å®šçš„ Docker stackã€‚

ä½¿ç”¨æ–¹å¼
--------
1. åœ¨å®¿ä¸»æ©Ÿå®‰è£ Dockerã€nvidiaâ€‘smiã€python3 ä»¥åŠ pipã€‚
2. æœ¬æ©ŸåŸ·è¡Œ `bash -c "python monitor.py"`
3. è‹¥æ¬²åœ¨å®¹å™¨å…§åŸ·è¡Œï¼Œè«‹åƒè€ƒä¸‹æ–¹æä¾›çš„ Dockerfile åŠåŸ·è¡ŒæŒ‡ä»¤ã€‚

ç’°å¢ƒè®Šæ•¸
---------
* CHECK_INTERVAL      : ç›£æ¸¬é–“éš”ï¼ˆç§’ï¼‰          (é è¨­ 60)
* MAX_UNHEALTHY       : æœªå›æ‡‰æ¬¡æ•¸æ–·ç·šé–€æª»      (é è¨­ 3)
* MAX_CPU_PERCENT     : CPU utilisation ä¸Šé™ï¼ˆ%ï¼‰   (é è¨­ 80.0)
* MAX_MEMORY_PERCENT  : Memory utilisation ä¸Šé™ï¼ˆ%ï¼‰(é è¨­ 80.0)
* MAX_IO_BYTES        : IO throughput ä¸Šé™ï¼ˆB/sï¼‰   (é è¨­ 100000000)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ä¸»è¦æ¨¡çµ„å°å…¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from datetime import datetime
import logging
import logging.handlers
import os
import sys
import subprocess
import time
import threading
from datetime import datetime
from typing import List, Tuple, Any, Dict, Optional

import docker
import requests
from docker.errors import DockerException, NotFound

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ æ—¥èªŒè¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å»ºç«‹å•Ÿå‹•æ™‚åˆ» (ISO 8601 ä½†æ›´é©åˆæª”å)
# ä¾‹: 20241023_125959.log
START_TS  = datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILE  = f"ollama_monitor_{START_TS}.log"
LOG_HOME = f"/nas2/kuang/MyPrograms/ollama/ollama_restart"
LOG_PATH  = os.path.join(LOG_HOME, LOG_FILE)
MAX_BYTES    = 2 * 1024 * 1024            # 2â€¯MB
BACKUP_COUNT = 5                          # ä¾‹å¦‚ .log, .log.1 â€¦ .log.5

# å…ˆå»ºç«‹ handler
file_handler = logging.handlers.RotatingFileHandler(
    LOG_PATH,
    mode="a",
    maxBytes=MAX_BYTES,
    backupCount=BACKUP_COUNT,
    encoding="utf-8",
    delay=False
)
file_handler.setLevel(logging.INFO)
# æ§åˆ¶å°ï¼ˆstdoutï¼‰handler
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)

# Formatter
formatter = logging.Formatter("%(asctime)s | %(levelname)8s | %(message)s")
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# æ¸›å°‘å‚³éå¤šå€‹ handler çš„è¤‡é›œåº¦ï¼šç›´æ¥å–å¾— __name__ çš„ logger
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)
log.addHandler(file_handler)
log.addHandler(console_handler)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ ç¨‹å¼çµ„æ…‹ï¼ˆå¯é€éç’°å¢ƒè®Šæ•¸è¦†å¯«ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHECK_INTERVAL: int = int(os.getenv("CHECK_INTERVAL", "120"))
MAX_UNHEALTHY: int = int(os.getenv("MAX_UNHEALTHY", "3"))
MAX_CPU_PERCENT: float = float(os.getenv("MAX_CPU_PERCENT", "80.0"))
MAX_MEMORY_PERCENT: float = float(os.getenv("MAX_MEMORY_PERCENT", "80.0"))
MAX_IO_BYTES: int = int(os.getenv("MAX_IO_BYTES", "100000000"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ å…§éƒ¨å·¥å…·å‡½å¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def dummy_inference(url: str, model: str, task: str = "å¯«ä¸€ç¯‡åšå£«ç­‰ç´šçš„è«–æ–‡ï¼Œè©•è«–èŠé½‹èªŒç•°ï¼Œè‡³å°‘1000å­—ã€‚") -> bool:
    """
    é€å‡ºä¸€å€‹ã€Œdummyã€æ¨è«–è«‹æ±‚ï¼Œè—‰æ­¤è§¸ç™¼ Ollama æœå‹™çš„ GPU è² è¼‰ã€‚

    Args:
        url:   å‘¼å«çš„ Ollama API æ¥é»ï¼ˆä¾‹ï¼šhttp://localhost:55080ï¼‰
        model: éœ€å‘¼å«çš„æ¨¡å‹åç¨±ï¼ˆä¾‹ï¼š`llama3.1:8b`ï¼‰
        task:  è¦æ¨è«–çš„æ–‡å­—ï¼Œé è¨­å€¼ç‚ºä¸­æ–‡æ¸¬è©¦å¥

    Returns:
        bool: è‹¥è«‹æ±‚æˆåŠŸ (HTTP 200)ï¼Œå›å‚³ Trueï¼›å¦å‰‡å›å‚³ False
    """
    try:
        resp = requests.post(
            f"{url}/api/chat",
            json={"model": model, "messages": [{"role": "user", "content": task}]},
            timeout=30,
        )
        return resp.status_code == 200
    except Exception as exc:   # pragma: no cover - ä¾‹å¤–æ™‚å³å¤±æ•—
        log.debug("dummy_inference å¤±æ•—: %s", exc)
        return False


def run_dummy(url: str, model: str) -> None:
    """
    åœ¨ç¨ç«‹åŸ·è¡Œç·’ä¸­å•Ÿå‹• dummy æ¨è«–ï¼Œé¿å…é˜»å¡ä¸»ç›£æ§è¿´åœˆã€‚
    """
    if dummy_inference(url, model):
        log.debug("Dummy æ¨è«–å·²é€å‡º")
    else:
        log.debug("Dummy æ¨è«–é€å‡ºå¤±æ•—ï¼è¶…æ™‚")


def _run_cmd(cmd: List[str], capture_output: bool = True) -> str:
    """åŸ·è¡Œ shell å‘½ä»¤ï¼Œä¸¦å›å‚³æ¨™æº–è¼¸å‡ºã€‚ä»»ä½•é 0 exit code çš†æœƒæ‹‹ä¾‹å¤–ã€‚"""
    result = subprocess.run(
        cmd,
        capture_output=capture_output,
        text=True,
        check=True,
    )
    return result.stdout.strip()


def gpu_usage_check(gpu_index: int = 0) -> Optional[int]:
    """
    è®€å–æŒ‡å®š GPU çš„ memory åˆ©ç”¨ç‡ï¼ˆ%ï¼‰
    è‹¥ç„¡ GPU æˆ–ç„¡æ³•è®€å–ï¼Œå›å‚³ Noneã€‚

    Args:
        gpu_index: GPU åœ¨ç³»çµ±ä¸­çš„ç´¢å¼•å€¼ (default 0)

    Returns:
        Optional[int]: memory utilisation (%)  æˆ– None
    """
    try:
        cmd = [
            "nvidia-smi",
            f"-i", str(gpu_index),
            "--query-gpu=utilization.memory",
            "--format=csv,noheader,nounits",
        ]
        out = _run_cmd(cmd)
        return int(out)
    except Exception as exc:  # pragma: no cover
        log.debug("gpu_usage_check åŸ·è¡Œå¤±æ•—: %s", exc)
        return None


def get_compute_pids(gpu_index: int = 0) -> List[int]:
    """
    å–å¾—ç›®å‰åœ¨ç‰¹å®š GPU ä¸Šé‹è¡Œçš„ PCIe æ²¹åŸ·è¡Œ PID
    """
    try:
        cmd = [
            "nvidia-smi",
            f"-i", str(gpu_index),
            "--query-compute-apps=pid",
            "--format=csv,noheader,nounits",
        ]
        raw = _run_cmd(cmd)
        return [int(pid) for pid in raw.splitlines() if pid.strip()]
    except Exception as exc:  # pragma: no cover
        log.debug("get_compute_pids åŸ·è¡Œå¤±æ•—: %s", exc)
        return []


def kill_pids(pids: List[int]) -> None:
    """
    çµ¦å®šæ¬²æ®ºæ‰çš„ PID æ¸…å–®ï¼Œé€ä¸€å‘¼å« `kill`ã€‚
    """
    for pid in pids:
        try:
            os.kill(pid, 15)  # SIGTERM
            log.info("Killed process %d on GPU", pid)
        except ProcessLookupError:
            log.debug("Process %d already dead", pid)
        except Exception as exc:
            log.warning("æ®ºæ‰ %d å¤±æ•—: %s", pid, exc)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5ï¸âƒ£ Docker ç›¸é—œå·¥å…·
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def remove_service(name: str) -> None:
    """
    é€šé dockerâ€‘py ç§»é™¤æŒ‡å®šæœå‹™ã€‚è‹¥ä¸å­˜åœ¨å‰‡å¿½ç•¥ã€‚

    Args:
        name: æœå‹™åç¨±
    """
    try:
        client = docker.from_env()
        svc = client.services.get(name)
        svc.remove()
        log.info("âœ”ï¸ Service '%s' removed.", name)
    except NotFound:
        log.info("âš ï¸ Service '%s' not found â€“ nothing to remove.", name)


def remove_services(names: List[str]) -> None:
    for svc_name in names:
        remove_service(svc_name)


def deploy_stack(compose_file: str, stack_name: str) -> None:
    """
    ä»¥å®¹å™¨å®¿ä¸»æ©Ÿçš„ docker CLI æ–¹å¼éƒ¨ç½² stackã€‚

    Args:
        compose_file : ç›®éŒ„å…§ dockerâ€‘compose.yml è·¯å¾‘
        stack_name   : Docker stack åç¨±
    """
    cmd = ["docker", "stack", "deploy", "-c", compose_file, stack_name]
    log.info("ğŸ”„ Deploying stack '%s' from %s â€¦", stack_name, compose_file)
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        log.info("stdout:\n%s", result.stdout.strip())
        log.info("stderr:\n%s", result.stderr.strip())
        log.info("âœ”ï¸ Stack deployed successfully.")
    except subprocess.CalledProcessError as exc:   # pragma: no cover
        log.error("Stack deploy failed: %s", exc)
        raise


def reset_services(yml: str, stack: str = "ollama") -> None:
    """
    å…ˆåˆªé™¤èˆŠæœå‹™ï¼Œå†é‡æ–°éƒ¨ç½² stackã€‚

    Args:
        yml   : dockerâ€‘compose.yml æª”æ¡ˆçµ•å°è·¯å¾‘
        stack : stack åç¨±ï¼Œé è¨­ç‚º 'ollama'
    """
    # 1ï¸âƒ£ åˆªé™¤èˆŠæœå‹™
    svc_names = [f"{stack}_{i}" for i in ["haproxy", "ollama"]]
    remove_services(svc_names)

    # 2ï¸âƒ£ é‡æ–°éƒ¨ç½²
    deploy_stack(yml, stack)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6ï¸âƒ£ ä¸»é‚è¼¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸‹é¢çš„åˆ—è¡¨æ˜¯ç’°å¢ƒçš„ç¡¬ç·¨ç¢¼è®Šé‡ï¼Œæ‚¨å¯ä»¥è‡ªè¡Œèª¿æ•´ç‚ºå¾é…ç½®æª”æˆ– env è®€å–ã€‚
stacks: List[str] = ["ollama", "ollama0"]
root_path: str = "/nas2/kuang/MyPrograms/ollama"
compose_files: List[str] = [
    f"{root_path}/docker-compose.yml",
    f"{root_path}/docker-compose.yml_llama3",
]
models: List[str] = ["gpt-oss:20b", "llama3.1:8b"]
ports: List[str] = ["55083", "55080"]
secs: List[int] = [15, 15]
base_url: str = "http://l40.sinotech-eng.com"
gpu_index: int = 0


def main() -> None:
    """
    ç¨‹å¼å…¥å£ï¼šé€ä¸€ç›£æ§è¨­å®šå¥½çš„ stackã€‚

    - ç›£æ§ GPU è¨˜æ†¶é«”å ç”¨
    - è‹¥æ²’æœ‰ç›¸æ‡‰çš„é€²ç¨‹ï¼Œè§¸ç™¼ dummy æ¨è«–ä»¥ç”¢ç”Ÿ GPU è² è¼‰
    - é•·æœŸç„¡ GPU ä½¿ç”¨æˆ–å¡é “å³é‡å•Ÿæœå‹™
    """
    while True:
        for stack_id, stack_name in enumerate(stacks):
            time.sleep(CHECK_INTERVAL)

            url = f"{base_url}:{ports[stack_id]}"
            model = models[stack_id]
            compose_file = compose_files[stack_id]
            pids = get_compute_pids(gpu_index)
            gpu_mem = gpu_usage_check(gpu_index)

            # â‘  æ²’æœ‰ä»»ä½•é€²ç¨‹æˆ– GPU ç„¡ä½¿ç”¨
            if not pids or gpu_mem is None or gpu_mem <= 0:
                # âŠ é€ dummy æ¨è«–ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰
                threading.Thread(target=run_dummy, args=(url, model), daemon=True).start()
                # â‹ è§€å¯Ÿ GPU è®ŠåŒ–
                for i in range(secs[stack_id]):
                    time.sleep(1)
                    gpu_mem2 = gpu_usage_check(gpu_index)

                    if gpu_mem2 >  gpu_mem:
                        # å¦‚æœ GPU å·²ç¶“æ¢å¾©ä½¿ç”¨ï¼Œæ¸…ç†åŸå…ˆå¯èƒ½æ®˜ç•™çš„é€²ç¨‹
                        pids = get_compute_pids(gpu_index)
                        kill_pids(pids)
                        break
                if gpu_mem2 == gpu_mem:
                    log.info("Ollama PID %s GPU æœªå ç”¨ï¼Œå°‡é‡å•Ÿæœå‹™", model)
                    reset_services(compose_file, stack_name)


if __name__ == "__main__":
    main()
